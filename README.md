## Sentence-Word-Embedding

### Project Topic
Investigating on different tensorflow pre-trained models on the topic of Universal
Sentence Encoder, comparing and analyzing how well these model to interpret groups of
input sentences with different nuances of words like star (movie/rock star, planet/sun, or
shape) or glasses (a glass of water, glasses to wear, a mirror, sheets of the material, or
even a person’s last name, etc.).

### Approach
We will start by looking up different feasible pre-trained models on tensorflow. We will
also look into many previous studies or examples to get familiar with the concept and
ways to interpret the query results by using the pre-trained models. One big challenge in
this project is to understand the output results from those pre-trained models and
language bias in the pretrained model dataset. There is also a need to come up with a
metric or standard rules to interpret the meaning and to evaluate the list of numbers after
applying the pretrain models. There are some models that we have conducted researches
on and here they are following:

### Models

### ● Universal Sentence Encoder
This model is mainly used for sentence embedding but can also multitask for things like
sentiment analysis, text classification, sentence similarity, etc,. The encoder has two
models, one is Transformer and the other is Deep Averaging Network(DAN). In order to
determine the inferential relationship between the same words in different sentences,
DAN would be a better choice.
